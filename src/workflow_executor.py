# Copyright 2022 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""Module to prepare arguments for execution and execute the Colabfold Workflow."""

import json
import os
import typing as T

from datetime import datetime
from pathlib import Path

from google.cloud import firestore
from google.cloud import storage
from google.cloud import workflows_v1beta
from google.cloud.workflows import executions_v1beta


def prepare_args_for_experiment(
    project_id: str,
    region: str,
    input_dir: str,
    image_uri: str,
    job_gcs_path: str,
    labels: dict,
    machine_type: str = 'n1-standard-4',
    cpu_milli: int = 8000,
    memory_mib: int = 30000,
    boot_disk_mib: int = 200000,
    gpu_type: str = "nvidia-tesla-t4",
    gpu_count: int = 1,
    job_gcsfuse_local_dir: str = '/mnt/disks/gcs/colabfold',
    parallelism: int = 8,
    template_mode: str = "none",
    use_cpu: bool = False,
    use_gpu_relax: bool = False,
    use_amber: bool = False,
    msa_mode: str = 'mmseqs2_uniref_env',
    model_type: str = 'auto',
    num_models: int = 5,
    num_recycle: int = 3,
    custom_template_path: str = None,
    overwrite_existing_results: bool = False,
    rank_by: str = 'auto',
    pair_mode: str = 'unpaired_paired',
    stop_at_score: int = 100,
    zip_results: bool = False
) -> T.Dict:
    '''
    Function to:
        - List all the FASTA files in a given GCS path
        - Create a document on Firestore for metadata tracking
        - Copy each FASTA file to an individual folder
        - Prepare and store the arguments for execution

    Arguments:
        project_id: str
            (Required) Project ID where the resources will be created and the
            execution will take place.
        region: str
            (Required) Region where the resources will be created.
        input_dir: str
            (Required) GCS path in the format of 'bucket_name/folder/subfolder/...'
            where the FASTA files were uploaded.
            Example:
                'mybucket/input-files'
        image_uri: str
            (Required) Image uri built during the setup of this solution.
            Example:
                'gcr.io/my_project/colabfold-batch'
        job_gcs_path: str
            (Required) GCS path to store the artifacts generated by the execution.
            Example:
                'my_bucket'
        labels: dict
            (Required) Labels to identify the job.
            Example:
                {'experiment_name':'T1050-protein','team':'drug-discovery'}
        machine_type: str = 'n1-standard-4'
            (Required) Machine type used to execute the Colabfold inference pipeline.
        cpu_milli: int = 8000
            (Required) Amount of CPU used to execute the job.
            Set this value to the maximum number of CPUs (in milli format)
        memory_mib: int = 30000
            (Required) Amount of memory used to execute the job.
            Set this value to the maximum memory available (in MiB format)
        boot_disk_mib: int = 200000
            (Required) Disk size allocated to the Instance which will execute the job.
        gpu_type: str = "nvidia-tesla-t4"
            (Required) GPU allocated to the instance type
        gpu_count: int = 1
            (Required) Number of GPUs to be allocated to the instance type
        job_gcsfuse_local_dir: str = '/mnt/disks/gcs/colabfold'
            (Required) Mount location of the GCSfuse.
            If you change this parameter, you must change the Workflow definition (yaml)
        parallelism: int = 8
            (Required) Maximum number of parallel jobs to be executed.
            We recommend set this number to the maximum GPU quota available in your project.

        >> The following parameters are related to the Colabfold inference pipeline.
        > These are the default parameters found in the original Colabfold repository.
            template_mode: str = "none"
            use_cpu: bool = False
            use_gpu_relax: bool = False
            use_amber: bool = False
            msa_mode: str = 'mmseqs2_uniref_env'
            model_type: str = 'auto'
            num_models: int = 5
            num_recycle: int = 3
            custom_template_path: str = None
            overwrite_existing_results: bool = False
            rank_by: str = 'auto'
            pair_mode: str = 'unpaired_paired'
            stop_at_score: int = 100
            zip_results: bool = False
    '''

    storage_client = storage.Client()
    db = firestore.Client()

    if template_mode == "pdb70":
        use_templates = True
        custom_template_path = None
    elif template_mode == "custom":
        use_templates = True
        custom_template_path = os.path.join(
            job_gcsfuse_local_dir,
            custom_template_path)
        custom_template_path
    else:
        custom_template_path = None
        use_templates = False
    
    args = {}
    args['project_id'] = project_id
    args['region'] = region
    args['image_uri'] = image_uri
    args['job_gcs_path'] = job_gcs_path
    args['parallelism'] = parallelism
    args['job_gcsfuse_local_dir'] = job_gcsfuse_local_dir
    args['machine_type'] = machine_type
    args['cpu_milli'] = cpu_milli
    args['memory_mib'] = memory_mib
    args['boot_disk_mib'] = boot_disk_mib
    args['gpu_type'] = gpu_type
    args['gpu_count'] = gpu_count

    # List all blobs in a GCS bucket/folder
    # Append blob information to a list
    if input_dir.endswith('/'):
        gcs_prefix = '/'.join(input_dir.split(sep='/')[1:])
    else:
        gcs_prefix = '/'.join(input_dir.split(sep='/')[1:]) + '/'

    blobs_listing = storage_client.list_blobs(
        job_gcs_path,
        prefix=gcs_prefix,
        delimiter='/')

    # Get only fasta files
    blobs = []
    for blob in blobs_listing:
        if Path(blob.name).suffix == '.fasta':
            blobs.append(blob)

    runners = []
    start_time = datetime.now().isoformat()

    doc_ref = db.collection('colabfold-experiments')

    input_bucket_name = input_dir.split(sep='/')[0]

    for blob in blobs:
        runner_args = {}
        exp_doc_ref = doc_ref.document()

        runner_args['job_id'] = f'job-colabfold-{exp_doc_ref.id.lower()}'
        runner_args['start_time'] = start_time
        runner_args['status'] = 'RUNNING'
        runner_args['job_gcs_path'] = job_gcs_path
        runner_args['labels'] = labels
        runner_args['filename'] = Path(blob.name).name
        runner_args['file_gcs_path'] = blob.name

        runner_args['use_templates'] = use_templates
        runner_args['use_amber'] = use_amber
        runner_args['msa_mode'] = msa_mode
        runner_args['model_type'] = model_type
        runner_args['num_models'] = num_models
        runner_args['num_recycle'] = num_recycle
        runner_args['rank_by'] = rank_by
        runner_args['pair_mode'] = pair_mode
        runner_args['stop_at_score'] = stop_at_score
        runner_args['zip_results'] = zip_results
        runner_args['firestore_ref'] = exp_doc_ref.id
        runner_args['custom_template_path'] = custom_template_path

        runner_args['output_gcs_path'] = os.path.join(
            'gs://',
            job_gcs_path,
            runner_args['job_id']            
        )

        runner_args['input_dir'] = os.path.join(
            job_gcsfuse_local_dir,
            runner_args['job_id']
        )
        runner_args['result_dir'] = runner_args['input_dir']

        source_bucket = storage_client.bucket(input_bucket_name)
        source_blob = source_bucket.blob(blob.name)
        destination_bucket = storage_client.bucket(job_gcs_path)

        source_bucket.copy_blob(
            source_blob, 
            destination_bucket,
            os.path.join(
                runner_args["job_id"], Path(blob.name).name)
        )

        # Write args to Firestore
        exp_doc_ref.set(runner_args, merge=True)

        commands = []
        commands.append(runner_args['input_dir'])
        commands.append(runner_args['result_dir'])
        commands.append(f'--stop-at-score={runner_args["stop_at_score"]}')
        commands.append(f'--num-recycle={runner_args["num_recycle"]}')
        commands.append(f'--num-models={runner_args["num_models"]}')
        commands.append(f'--msa-mode={runner_args["msa_mode"]}')
        commands.append(f'--model-type={runner_args["model_type"]}')
        commands.append(f'--rank={runner_args["rank_by"]}')
        commands.append(f'--pair-mode={runner_args["pair_mode"]}')

        if custom_template_path:
            commands.append(f'--custom-template-path={custom_template_path}')
        if use_amber:
            commands.append('--amber')
        if use_templates:
            commands.append('--templates')
        if use_cpu:
            commands.append('--cpu')
        if use_gpu_relax:
            commands.append('--use-gpu-relax')
        if overwrite_existing_results:
            commands.append('--overwrite-existing-results')

        # commands = ' '.join(commands)
        runner_args['commands'] = commands

        runners.append(runner_args)

    args['runners'] = runners

    return args


def execute_workflow( 
    workflow_name: str,
    args: T.Dict
):
    """Submit a Cloud Workflows to process the Alphafold inference pipeline

    Args:
        workflow_name (str):
            Required. Name of the Cloud Workflows deployed in the setup stage.
        args (dict):
            Required. All the prepared arguments to execute the Workflows.
    Returns:
        (str) Full name of the Workflows path.
    Raises:
        None
    """
    # Set up API clients.
    execution_client = executions_v1beta.ExecutionsClient()
    workflows_client = workflows_v1beta.WorkflowsClient()

    # Construct the fully qualified location path.
    parent = workflows_client.workflow_path(
        args['project_id'], args['region'], workflow_name)

    execution = executions_v1beta.Execution(argument=json.dumps(args))
    exec_request = executions_v1beta.CreateExecutionRequest(
        parent = parent,
        execution = execution
    )

    # Execute the workflow.
    response = execution_client.create_execution(request=exec_request)

    print(f"Created execution: {response.name}")

    return response.name
